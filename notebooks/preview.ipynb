{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo\n",
    "- try to see if i can nn the tot, expecting large variance for that unknown tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas\n",
    "- transform of outputs\n",
    "- pca outputs, to reduce dimensionality as needed\n",
    "- pca inputs, to reduce complexity\n",
    "- test for signal\n",
    "  - 1 vs 2 nearest neighbor outputs\n",
    "  - chl-a retrieval algorithms\n",
    "- dealing with unbalanced data (are they unbalanced?)\n",
    "- try classification only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.2.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.1/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.1/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.1/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js\", \"https://cdn.holoviz.org/panel/1.2.1/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import warnings\n",
    "import datetime as dt\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from scipy.stats import zscore\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import param as p\n",
    "import xarray as xr\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "# import tensorflow_probability as tfp\n",
    "\n",
    "# from re_nobm_pcc import preprocess\n",
    "from re_nobm_pcc import DATADIR, TAXA, WAVELENGTH\n",
    "from re_nobm_pcc import kit\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
    "hv.opts.defaults(\n",
    "    hv.opts.Curve(active_tools=[]),\n",
    "    hv.opts.Image(active_tools=[]),\n",
    "    hv.opts.Scatter(active_tools=[]),\n",
    "    hv.opts.HexTiles(active_tools=[], tools=[\"hover\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_name = {\n",
    "    \"alk\": \"alkalinity\",\n",
    "    \"cdc\": \"colored dissolved carbon\",\n",
    "    \"chl\": \"chlorophytes\",\n",
    "    \"coc\": \"coccolithophores\",\n",
    "    \"cya\": \"cyanobacteria\",\n",
    "    \"dia\": \"diatoms\",\n",
    "    \"dic\": \"dissolved organic carbon\",\n",
    "    \"din\": \"dinoflagellate\",\n",
    "    \"doc\": \"dissolved organic carbon\",\n",
    "    \"dtc\": \"dissolved total carbon\",\n",
    "    \"fco\": \"carbon dioxide flux\",\n",
    "    \"h\": \"mixed layer depth\",\n",
    "    \"irn\": \"iron\",\n",
    "    \"pco\": \"carbon dioxide concentration\",\n",
    "    \"pha\": \"phaeocystis\",\n",
    "    \"pic\": \"particulate inorganic carbon\",\n",
    "    \"pp\": \"phytoplankton primary productivity\",\n",
    "    \"tpp\": \"total primary productivity\",\n",
    "    \"rnh\": \"ammonium\",\n",
    "    \"rno\": \"nitrate\",\n",
    "    \"s\": \"salinity\",\n",
    "    \"t\": \"temperature\",\n",
    "    \"tot\": \"total chlorophyl\",\n",
    "    \"zoo\": \"zooplankton\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OASIM model requires absorption and backscattering for each phytoplankton group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/oasim_param/dia1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m TAXA:\n\u001b[1;32m      3\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../data/oasim_param/\u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m1.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_table(path, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49m{\u001b[39m0\u001b[39;49m: \u001b[39mint\u001b[39;49m})\n\u001b[1;32m      5\u001b[0m     df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mwavelength\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mabsorption\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mscattering\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     da \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mset_index(\u001b[39m\"\u001b[39m\u001b[39mwavelength\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto_xarray()\u001b[39m.\u001b[39mexpand_dims(\u001b[39m\"\u001b[39m\u001b[39mcomponent\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/re-nobm-pcc-IR-aANdM-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1282\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1269\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1270\u001b[0m     dialect,\n\u001b[1;32m   1271\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1279\u001b[0m )\n\u001b[1;32m   1280\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/re-nobm-pcc-IR-aANdM-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/re-nobm-pcc-IR-aANdM-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/re-nobm-pcc-IR-aANdM-py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/re-nobm-pcc-IR-aANdM-py3.11/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/oasim_param/dia1.txt'"
     ]
    }
   ],
   "source": [
    "ds = []\n",
    "for item in TAXA:\n",
    "    path = f\"../data/oasim_param/{item}1.txt\"\n",
    "    df = pd.read_table(path, sep=\"\\t\", dtype={0: int})\n",
    "    df.columns = (\"wavelength\", \"absorption\", \"scattering\")\n",
    "    da = df.set_index(\"wavelength\").to_xarray().expand_dims(\"component\")\n",
    "    da[\"component\"] = [item]\n",
    "    ds.append(da)\n",
    "ds = xr.concat(ds, \"component\")\n",
    "(\n",
    "    ds.hvplot.line(x=\"wavelength\", y=\"absorption\", by=\"component\")\n",
    "    + ds.hvplot.line(x=\"wavelength\", y=\"scattering\", by=\"component\")\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NOBM data provided by Cecile contains the ocean constituents that are sufficient inputs for the OASIM Fortran library to calculte Rrs.\n",
    "\n",
    "Below 350nm however, there is no phytoplankton absorption data so those Rrs values should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    f\"../data/rrs_day/rrs{1998+i}{1+j:02}.nc\" for j in range(12) for i in range(24)\n",
    "]\n",
    "ds = xr.open_mfdataset(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME move to re_nobm_pcc.simulate.py once oasim is fixed\n",
    "ds = ds.roll({\"lon\": ds.sizes[\"lon\"] // 2})\n",
    "coords = xr.Dataset(\n",
    "    coords={\n",
    "        \"lon\": np.linspace(-180, 180, ds.sizes[\"lon\"]),\n",
    "        \"lat\": np.linspace(-84, 71.4, ds.sizes[\"lat\"]),\n",
    "    },\n",
    ")\n",
    "ds = xr.merge((ds.drop_vars((\"lon\", \"lat\")), coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dashboard(p.Parameterized):\n",
    "    # part of the GUI\n",
    "    date = p.Date(dt.date(1998, 1, 1))\n",
    "    h2o = p.Selector(\n",
    "        [\"tot\", \"dtc\", \"pic\", \"cdc\", \"t\", \"s\"], label=\"Ocean Property Variable\"\n",
    "    )\n",
    "    phy = p.Selector(ds[\"component\"].values.tolist(), label=\"Phytoplankton Group\")\n",
    "    # needed as dependencies, not part of the GUI\n",
    "    data = p.ClassSelector(xr.Dataset)\n",
    "    stream = hv.streams.Tap(x=0, y=0)\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tap = xr.DataArray(\n",
    "            np.empty((ds.sizes[\"wavelength\"], 0), dtype=ds[\"rrs\"].dtype),\n",
    "            dims=(\"wavelength\", \"tap\"),\n",
    "            name=\"rrs\",\n",
    "        )\n",
    "\n",
    "    @p.depends(\"date\", watch=True, on_init=True)\n",
    "    def _load_date(self):\n",
    "        self.data = ds.sel({\"date\": np.datetime64(self.date)}).load()\n",
    "\n",
    "    @p.depends(\"data\", \"h2o\")\n",
    "    def plt_h2o(self):\n",
    "        da = self.data[self.h2o]\n",
    "        return da.hvplot.image(x=\"lon\", y=\"lat\", clabel=self.h2o, title=\"\")\n",
    "\n",
    "    @p.depends(\"data\", \"phy\")\n",
    "    def plt_phy(self):\n",
    "        da = self.data[\"phy\"].sel({\"component\": self.phy})\n",
    "        plt = da.hvplot.image(x=\"lon\", y=\"lat\", clabel=self.phy, title=\"\")\n",
    "        self.stream.source = plt\n",
    "        return plt\n",
    "\n",
    "    @p.depends(\"stream.x\", \"stream.y\")\n",
    "    def plt_rrs(self):\n",
    "        da = self.data[\"rrs\"].sel(\n",
    "            {\"lon\": self.stream.x, \"lat\": self.stream.y},\n",
    "            method=\"nearest\",\n",
    "        )\n",
    "        da = da.expand_dims(\"tap\")\n",
    "        self.tap = xr.concat((self.tap, da), dim=\"tap\")\n",
    "        return self.tap.hvplot(x=\"wavelength\", by=\"tap\", title=\"\")\n",
    "\n",
    "\n",
    "dash = Dashboard(name=\"NOBM Variables and Computed Rrs\")\n",
    "pn.Column(\n",
    "    pn.panel(\n",
    "        dash.param,\n",
    "        parameters=[\"date\", \"h2o\", \"phy\"],\n",
    "        widgets={\"date\": pn.widgets.DatePicker},\n",
    "    ),\n",
    "    dash.plt_h2o,\n",
    "    dash.plt_phy,\n",
    "    dash.plt_rrs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chl-a OC4 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OC4 (SeaWiFS) from https://oceancolor.gsfc.nasa.gov/atbd/chlor_a/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.32814, -3.20725, 3.22969, -1.36769, -0.81739]\n",
    "blue = [443, 489, 510]\n",
    "green = 555\n",
    "\n",
    "dim = \"wavelength\"\n",
    "da = xr.DataArray(\n",
    "    np.arange(len(WAVELENGTH)),\n",
    "    coords={dim: ds[dim].loc[WAVELENGTH[0] : WAVELENGTH[-1]]},\n",
    ")\n",
    "blue = da.sel({dim: blue}, method=\"nearest\").values.tolist()\n",
    "green = da.sel({dim: green}, method=\"nearest\").values.item()\n",
    "\n",
    "a = tf.expand_dims(tf.constant(a), 1)\n",
    "blue, green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def log_blue_green_ratio(x, y):\n",
    "    return (\n",
    "        tf.expand_dims(\n",
    "            tf.experimental.numpy.log10(\n",
    "                tf.reduce_max(tf.gather(x, blue, axis=1), axis=1) / x[:, green]\n",
    "            ),\n",
    "            axis=1,\n",
    "        ),\n",
    "        tf.expand_dims(tf.experimental.numpy.log10(tf.reduce_sum(y, axis=1)), axis=1),\n",
    "    )\n",
    "\n",
    "\n",
    "batch_size = 2**10\n",
    "tfds_rrs_day = tfds.builder(\"rrs_day_tfds\", data_dir=DATA_DIR)\n",
    "train, test = tfds_rrs_day.as_dataset(\n",
    "    split=[\"split[:8%]\", \"split[9%:10%]\"], as_supervised=True\n",
    ")\n",
    "train_size = train.cardinality()\n",
    "test_size = test.cardinality()\n",
    "train = train.batch(batch_size).cache()\n",
    "test = test.batch(batch_size).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x, log_y = test.map(log_blue_green_ratio).rebatch(test_size).get_single_element()\n",
    "log_y = log_y[:, 0].numpy()\n",
    "log_y_hat = (log_x ** tf.range(5, dtype=np.float32) @ a)[:, 0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y - log_y_hat) ** 2).sum() / ((log_y - log_y.mean()) ** 2).sum()\n",
    "print(f\"R2: {R2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hv.Scatter(\n",
    "        (log_y_hat[: 2**16], log_y[: 2**16]),\n",
    "        kdims=\"prediction\",\n",
    "        vdims=\"truth\",\n",
    "    )\n",
    "    + hv.HexTiles(\n",
    "        (log_y_hat, log_y),\n",
    "        kdims=[\"prediction\", \"truth\"],\n",
    "    ).opts(logz=True)\n",
    ") * hv.Slope(1, 0).opts(color=\"black\", line_width=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Lambda(lambda x: x ** tf.range(1, 5, dtype=np.float32)),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "network.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = network.fit(\n",
    "    train.map(log_blue_green_ratio).shuffle(batch_size * 4),\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x, log_y = test.map(log_blue_green_ratio).rebatch(test_size).get_single_element()\n",
    "log_y_hat = network(log_x)[:, 0].numpy()\n",
    "log_y = log_y[:, 0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y - log_y_hat) ** 2).sum() / ((log_y - log_y.mean()) ** 2).sum()\n",
    "print(f\"R2: {R2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hv.Scatter(\n",
    "        (log_y_hat[: 2**16], log_y[: 2**16]),\n",
    "        kdims=\"prediction\",\n",
    "        vdims=\"truth\",\n",
    "    )\n",
    "    + hv.HexTiles(\n",
    "        (log_y_hat, log_y),\n",
    "        kdims=[\"prediction\", \"truth\"],\n",
    "    ).opts(logz=True)\n",
    ") * hv.Slope(1, 0).opts(color=\"black\", line_width=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def four_wavelengths(x, y):\n",
    "    return (\n",
    "        tf.experimental.numpy.log10(\n",
    "            tf.concat(\n",
    "                (tf.gather(x, blue, axis=1), tf.gather(x, [green], axis=1)),\n",
    "                axis=1,\n",
    "            )\n",
    "        ),\n",
    "        tf.experimental.numpy.log10(tf.reduce_sum(y, axis=1)),\n",
    "    )\n",
    "\n",
    "\n",
    "network = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, \"relu\"),\n",
    "        tf.keras.layers.Dense(64, \"relu\"),\n",
    "        tf.keras.layers.Dense(64, \"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "network.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = network.fit(\n",
    "    train.map(four_wavelengths).shuffle(batch_size * 4),\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x, log_y = test.map(four_wavelengths).rebatch(test_size).get_single_element()\n",
    "log_y_hat = network(log_x)[:, 0].numpy()\n",
    "log_y = log_y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y - log_y_hat) ** 2).sum() / ((log_y - log_y.mean()) ** 2).sum()\n",
    "print(f\"R2: {R2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hv.Scatter(\n",
    "        (log_y_hat[: 2**16], log_y[: 2**16]),\n",
    "        kdims=\"prediction\",\n",
    "        vdims=\"truth\",\n",
    "    )\n",
    "    + hv.HexTiles(\n",
    "        (log_y_hat, log_y),\n",
    "        kdims=[\"prediction\", \"truth\"],\n",
    "    ).opts(logz=True)\n",
    ") * hv.Slope(1, 0).opts(color=\"black\", line_width=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp, loc scale out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, \"relu\"),\n",
    "        tf.keras.layers.Dense(64, \"relu\"),\n",
    "        tf.keras.layers.Dense(64, \"relu\"),\n",
    "        tf.keras.layers.Dense(2),\n",
    "        tfp.layers.IndependentNormal(1),\n",
    "    ]\n",
    ")\n",
    "network.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=lambda y, model: tf.reduce_sum(-model.log_prob(y)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = network.fit(\n",
    "    train.map(four_wavelengths).shuffle(batch_size * 4),\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x, log_y = test.map(four_wavelengths).rebatch(test_size).get_single_element()\n",
    "log_y_model = network(log_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = log_y_model.stddev() / tf.abs(log_y_model.mean()) < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_y_hat = tf.boolean_mask(log_y_model.sample(), idx).numpy()\n",
    "log_y = tf.boolean_mask(log_y, idx[:, 0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y - log_y_hat) ** 2).sum() / ((log_y - log_y.mean()) ** 2).sum()\n",
    "print(f\"R2: {R2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hv.Scatter(\n",
    "        (log_y_hat[: 2**16], log_y[: 2**16]),\n",
    "        kdims=\"prediction\",\n",
    "        vdims=\"truth\",\n",
    "    )\n",
    "    + hv.HexTiles(\n",
    "        (log_y_hat, log_y),\n",
    "        kdims=[\"prediction\", \"truth\"],\n",
    "    ).opts(logz=True)\n",
    ") * hv.Slope(1, 0).opts(color=\"black\", line_width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE transform to standard normal and plot ecdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectum with Taxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(train.shuffle(32).as_numpy_iterator())\n",
    "(hv.Curve(x) + hv.Bars(y)).opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Outdated Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUK41jPxQHeT"
   },
   "source": [
    "## Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZUWOMYQgYBK"
   },
   "source": [
    "The features and labels are both model output from NASA GMAO using the [NOBM and OASIM](https://gmao.gsfc.nasa.gov/gmaoftp/NOBM) models. The labels are four phytoplankton chlorophyll densities output by NOBM. The features are normalized water leaving radiances output by OASIM, using the NOBM model as input."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocess = importlib.reload(preprocess)\n",
    "kit = importlib.reload(kit)\n",
    "HyperLwn = preprocess.HyperLwn\n",
    "PhytoChl = preprocess.PhytoChl\n",
    "\n",
    "sample = xr.open_dataset(kit.DATA_DIR/'sample.nc')\n",
    "sample['pxl'] = range(sample.sizes['pxl'])\n",
    "sample['labels'] = (\n",
    "    sample[kit.TAXA]\n",
    "    .to_array(dim='component')\n",
    "    .transpose('pxl', 'component', ...)\n",
    ")\n",
    "sample_n = (sample - sample.mean('pxl')) #/sample.std('pxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0gHiexEgq3T"
   },
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CNGKtURguXF"
   },
   "source": [
    "One NetCDF file contains all the predictor data. Note that the `FillValue` attribute is not set to `9.99e11` in the netCDF file (Cecile will fix in next version). There are no explicit coordinates given; they are documented as attributes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ncdump -h {os.environ['PWD']}/data/nobm/HyperLwn.R2014.nc4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nonnull_grid = int((~HyperLwn.isel(wavelength=0, month=0).isnull()).sum())\n",
    "Markdown(f\"\"\"\n",
    "Variable `HyperLwn` has non-null values at {nonnull_grid:,} pixels for each month\n",
    "and wavelength.\n",
    "\n",
    "In total, that gives {nonnull_grid * HyperLwn.sizes['month']:,} samples (that are highly non-independent!).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nonnull = int(HyperLwn.size - HyperLwn.isnull().sum())\n",
    "Markdown(f\"\"\"\n",
    "Augmented with coordinates, variable `HyperLwn` is a xarray.DataArray with {nonnull:,} values.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HyperLwn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpkjtJG-iRTW"
   },
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFz1ooc6iaCY"
   },
   "source": [
    "Each of twelve NetCDF files contain a month of NOBM model output. The first is representative. Unlike the HyperLwn file, this one contains coordinates."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ncdump -h {os.environ['PWD']}/data/nobm/monthly/mon200701.R2014.nc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PhytoChl` xarray.Dataset includes the different phytoplankton groups as variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PhytoChl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLvJkZOpjPzn"
   },
   "source": [
    "## Plot your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaHaKBVujTGO"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The radiances currently make a nice map, but the data should be more sparsely sampled."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dmap = (\n",
    "    HyperLwn\n",
    "    .sel(month=[2, 6, 10], wavelength=[465, 665], method='nearest')\n",
    "    .hvplot.image(\n",
    "        groupby=['month', 'wavelength'],\n",
    "        subplots=True,\n",
    "        clabel='Lwn (mW cm-2 microm-1 sr-1)',\n",
    "        rasterize=True,\n",
    "    )\n",
    "    .opts(shared_axes=False)\n",
    ")\n",
    "dmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few \"typical\" hyperspectral radiances."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dmap = (\n",
    "    HyperLwn\n",
    "    .sel({'lon': -120, 'lat': -15, 'month': [2, 6, 10]}, method='nearest')\n",
    "    .hvplot\n",
    "    .line(by='month', ylabel='Lwn')\n",
    "    # * hv.Slope(0, -0.2).options(color=hv.dim('wavelength'))\n",
    ")\n",
    "dmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean centered radiances and corresponding phytoplankton abundances."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pxl = [4, 34, 53, 283]\n",
    "grays = ['#000000', '#444444', '#777777', '#aaaaaa']\n",
    "pigments = ['#47AC5F', '#FBEC2C', '#F884AB', '#E93429']\n",
    "line = (\n",
    "    sample['features'].sel(pxl=pxl)\n",
    "    .hvplot\n",
    "    .line(x='wavelength', by='pxl', ylabel='Lwn', legend=True)\n",
    "    .options('Curve', fontscale=1.4, color=hv.Cycle(grays))\n",
    "    .options('NdOverlay', legend_position='top_right')\n",
    ")\n",
    "(\n",
    "    line\n",
    "    + (\n",
    "        sample['labels']\n",
    "        .reset_coords(drop=True)\n",
    "        .isel(pxl=pxl)\n",
    "        .hvplot.bar(by='component')\n",
    "        .options('Bars', fontscale=1.4, color=hv.Cycle(pigments))\n",
    "    )\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD to reduce the wavelength dimension to `k` vectors accounting for the most variation in the features. The singular values are:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k = 5\n",
    "scores, s, vectors = kit.svd(sample_n['features'], dim='wavelength', k=k)\n",
    "list(s.round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding vectors:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectors.hvplot.line(x='wavelength', by='pc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix of univariate (diagonal) and bivariate (off-diagonal) histograms of the `scores`, or coefficients generating each wavelength by linear combination of the `vectors` above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\n",
    "    hvplot.scatter_matrix(\n",
    "        scores.to_dataset(dim='pc').to_dataframe(),\n",
    "        chart='hexbin',\n",
    "        gridsize=16,\n",
    "    )\n",
    "    .opts(hv.opts.HexTiles(cmap='Viridis', tools=['hover']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEXjgfVPjspm"
   },
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A map of the phytoplankton labels in `PhytoChl` at one month."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\n",
    "    PhytoChl\n",
    "    .sel(month=[2, 5, 8, 11])\n",
    "    .hvplot.image(\n",
    "        z=kit.TAXA,\n",
    "        groupby=['month'],\n",
    "        subplots=True,\n",
    "        clabel='chl-a',\n",
    "        rasterize=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the four phytoplankton groups."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample['labels_p'] = (sample['labels'].dims, kit.ecdf(sample['labels']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\n",
    "    sample[['labels', 'labels_p']]\n",
    "    .drop_vars('pxl')\n",
    "    .hvplot\n",
    "#    .line(x='labels', y='labels_p', by='component')\n",
    "#    .opts(hv.opts.Curve(interpolation='steps-pre'))\n",
    "    .scatter(x='labels', y='labels_p', by='component', xlabel='chl-a', ylabel='probability')\n",
    "    .opts(title='ECDF of phytoplankton by component')\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scores, s, vectors = kit.svd(sample_n['labels'], dim='component')\n",
    "s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.cov(scores, rowvar=False).round(8)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels = xr.Dataset({\n",
    "    'scores': scores,\n",
    "    'scores_p': (scores.dims, kit.ecdf(scores)),\n",
    "})\n",
    "(\n",
    "    labels[['scores', 'scores_p']]\n",
    "    .hvplot\n",
    "#    .line(x='labels', y='labels_p', by='component')\n",
    "#    .opts(hv.opts.Curve(interpolation='steps-pre'))\n",
    "    .scatter(x='scores', y='scores_p', by='pc', xlabel='score', ylabel='probability')\n",
    "    .opts(title='ECDF of phytoplankton PCA by component')\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\n",
    "    hvplot.scatter_matrix(\n",
    "        scores.to_dataset(dim='pc').to_dataframe(),\n",
    "        chart='hexbin',\n",
    "        gridsize=16,\n",
    "    )\n",
    "    .opts(hv.opts.HexTiles(cmap='Viridis', tools=['hover']))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "re-nobm-pcc-IR-aANdM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
