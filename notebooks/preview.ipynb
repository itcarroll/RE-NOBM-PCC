{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo\n",
    "- try to see if i can nn the tot, expecting large variance for that unknown tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas\n",
    "- transform of outputs\n",
    "- pca outputs, to reduce dimensionality as needed\n",
    "- pca inputs, to reduce complexity\n",
    "- test for signal\n",
    "  - 1 vs 2 nearest neighbor outputs\n",
    "  - chl-a retrieval algorithms\n",
    "- dealing with unbalanced data (are they unbalanced?)\n",
    "- try classification only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import warnings\n",
    "import datetime as dt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from scipy.stats import zscore\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import param as p\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "#from re_nobm_pcc import preprocess\n",
    "from re_nobm_pcc import DATA_DIR, WAVELENGTH, TAXA\n",
    "from re_nobm_pcc import kit\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "hv.opts.defaults(\n",
    "    hv.opts.Bars(active_tools=[]),\n",
    "    hv.opts.Curve(active_tools=[]),\n",
    "    hv.opts.Image(active_tools=[]),\n",
    "    hv.opts.Scatter(active_tools=[]),\n",
    "    hv.opts.HexTiles(active_tools=[], tools=['hover']),\n",
    ")\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OASIM model requires absorption and backscattering for each phytoplankton group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "for item in ['dia', 'chl', 'cya', 'coc', 'pha', 'din']:\n",
    "    path = f'../data/oasim_param/{item}1.txt'\n",
    "    df = pd.read_table(path, sep='\\t', dtype={0: int})\n",
    "    df.columns = ('wavelength', 'absorption', 'scattering')\n",
    "    da = df.set_index('wavelength').to_xarray().expand_dims('component')\n",
    "    da['component'] = [item]\n",
    "    ds.append(da)\n",
    "ds = xr.concat(ds, 'component')\n",
    "(\n",
    "    ds.hvplot.line(x='wavelength', y='absorption', by='component')\n",
    "    + ds.hvplot.line(x='wavelength', y='scattering', by='component')\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NOBM data provided by Cecile contains the ocean constituents that are sufficient inputs for the OASIM Fortran library to calculte Rrs.\n",
    "\n",
    "Below 350nm however, there is no phytoplankton absorption data so those Rrs values should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [f'../data/rrs_day/rrs{1998+i}{1+j:02}.nc' for i in range(23, 24) for j in range(4, 5)]\n",
    "ds = xr.open_mfdataset(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME move to re_nobm_pcc.simulate.py once oasim is fixed\n",
    "ds = ds.roll({'lon': ds.sizes['lon'] // 2})\n",
    "coords = xr.Dataset(\n",
    "    coords={\n",
    "        'lon': np.linspace(-180, 180, ds.sizes['lon']),\n",
    "        'lat': np.linspace(-84, 71.4, ds.sizes['lat'])\n",
    "    },\n",
    ")\n",
    "ds = xr.merge((ds.drop_vars(('lon', 'lat')), coords))\n",
    "ds = ds.sel({'wavelength': slice(WAVELENGTH[0], WAVELENGTH[-1])})\n",
    "(ds['date'].min().data, ds['date'].max().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dashboard(p.Parameterized):\n",
    "    \n",
    "    # part of the GUI\n",
    "    date = p.Date(dt.date(2021, 5, 8))\n",
    "    h2o = p.Selector(['tot', 'dtc', 'pic', 'cdc', 't', 's'], label='Ocean Property Variable')\n",
    "    phy = p.Selector(ds['component'].values.tolist(), label='Phytoplankton Group')\n",
    "    # needed as dependencies, not part of the GUI\n",
    "    data = p.ClassSelector(xr.Dataset)\n",
    "    stream = hv.streams.Tap(x=0, y=0)\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tap_rrs = xr.DataArray(\n",
    "            np.empty((ds.sizes['wavelength'], 0), dtype=ds['rrs'].dtype),\n",
    "            dims=('wavelength', 'tap'),\n",
    "            name='rrs',\n",
    "        )\n",
    "        self.tap_phy = xr.DataArray(\n",
    "            np.empty((ds.sizes['component'], 0), dtype=ds['phy'].dtype),\n",
    "            dims=('component', 'tap'),\n",
    "            name='phy',\n",
    "        )\n",
    "    \n",
    "    @p.depends('date', watch=True, on_init=True)\n",
    "    def _load_date(self):\n",
    "        self.data = ds.sel({'date': np.datetime64(self.date)}).load()\n",
    "        \n",
    "    @p.depends('data', 'h2o')\n",
    "    def plt_h2o(self):\n",
    "        da = self.data[self.h2o]\n",
    "        return da.hvplot.image(x='lon', y='lat', clabel=self.h2o, title='')\n",
    "    \n",
    "    @p.depends('data', 'phy')\n",
    "    def plt_phy(self):\n",
    "        da = self.data['phy'].sel({'component': self.phy})\n",
    "        plt = da.hvplot.image(x='lon', y='lat', clabel=self.phy, title='')\n",
    "        self.stream.source = plt\n",
    "        return plt\n",
    "\n",
    "    @p.depends('stream.x', 'stream.y')\n",
    "    def plt_rrs(self):\n",
    "        da = self.data['rrs'].sel(\n",
    "            {'lon': self.stream.x, 'lat': self.stream.y},\n",
    "            method='nearest',\n",
    "        )\n",
    "        da = da.expand_dims('tap')\n",
    "        self.tap_rrs = xr.concat((self.tap_rrs, da), dim='tap')\n",
    "        return self.tap_rrs.hvplot(x='wavelength', by='tap', title='', color='black', fontscale=1.4)\n",
    "\n",
    "    @p.depends('stream.x', 'stream.y')\n",
    "    def plt_phy_bar(self):\n",
    "        da = self.data['phy'].sel(\n",
    "            {'lon': self.stream.x, 'lat': self.stream.y},\n",
    "            method='nearest',\n",
    "        )\n",
    "        da = da.expand_dims('tap')\n",
    "        da = xr.concat((self.tap_phy, da), dim='tap')\n",
    "        self.tap_phy = da.drop_vars(('lon', 'lat', 'date'))\n",
    "        return self.tap_phy.hvplot.bar(x='component', by='tap', color='component', title='', fontscale=1.4)\n",
    "\n",
    "\n",
    "dash = Dashboard(name='NOBM Variables and Computed Rrs')\n",
    "pn.Column(\n",
    "    pn.panel(dash.param, parameters=['date', 'h2o', 'phy'], widgets={'date': pn.widgets.DatePicker}),\n",
    "    dash.plt_h2o,\n",
    "    dash.plt_phy,\n",
    "    dash.plt_rrs,\n",
    "    dash.plt_phy_bar,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy = TAXA[5]\n",
    "da = ds['phy'].sel({'component': phy, 'date': '2021-05-08'})\n",
    "plt = da.hvplot.image(x='lon', y='lat', title=phy, clabel='chl-a', aspect=288/234)\n",
    "plt.options(fontscale=1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chl-a OC4 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OC4 (SeaWiFS) from https://oceancolor.gsfc.nasa.gov/atbd/chlor_a/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.32814, -3.20725, 3.22969, -1.36769, -0.81739]\n",
    "blue = [443, 489, 510]\n",
    "green = 555\n",
    "\n",
    "dim = 'wavelength'\n",
    "da = xr.DataArray(\n",
    "    np.arange(len(WAVELENGTH)),\n",
    "    coords={dim: ds[dim].loc[WAVELENGTH[0]:WAVELENGTH[-1]]},\n",
    ")\n",
    "blue = da.sel({dim: blue}, method='nearest').values.tolist()\n",
    "green = da.sel({dim: green}, method='nearest').values.item()\n",
    "\n",
    "a = tf.expand_dims(tf.constant(a), 1)\n",
    "blue, green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def log_blue_green_ratio(x, y):\n",
    "    return (\n",
    "        tf.expand_dims(tf.experimental.numpy.log10(\n",
    "            tf.reduce_max(tf.gather(x, blue, axis=1), axis=1) / x[:, green]\n",
    "        ), axis=1),\n",
    "        tf.expand_dims(tf.experimental.numpy.log10(\n",
    "            tf.reduce_sum(y, axis=1)\n",
    "        ), axis=1),\n",
    "    )\n",
    "\n",
    "batch_size = 2 ** 10\n",
    "tfds_rrs_day = tfds.builder('rrs_day_tfds', data_dir=DATA_DIR)\n",
    "train, test = tfds_rrs_day.as_dataset(split=['split[7:8%]', 'split[9%:10%]'], as_supervised=True)\n",
    "train_size = train.cardinality()\n",
    "test_size = test.cardinality()\n",
    "train = train.batch(batch_size).cache()\n",
    "test = test.batch(batch_size).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x, log_y = test.map(log_blue_green_ratio).rebatch(test_size).get_single_element()\n",
    "log_y = log_y[:, 0].numpy()\n",
    "log_y_hat = (log_x ** tf.range(5, dtype=np.float32) @ a)[:, 0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y - log_y_hat)**2).sum()/((log_y - log_y.mean())**2).sum()\n",
    "print(f'R2: {R2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = hv.Dimension('prediction', range=(-2.5, 2))\n",
    "true = hv.Dimension('truth', range=(-16, 3))\n",
    "count = hv.Dimension('count', range=(1, 10**6))\n",
    "plt = (\n",
    "    hv.HexTiles((log_y_hat, log_y), kdims=[pred, true], vdims=count)\n",
    "    * hv.Slope(1, 0)\n",
    ")\n",
    "plt = plt.options('HexTiles', logz=True, colorbar=True, aspect=1, fontscale=1.4)\n",
    "plt = plt.options('Slope', color='red', line_width=2)\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-trained coeffs but still OC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: x ** tf.range(1, 5, dtype=np.float32)),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "network.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = network.fit(\n",
    "    train.map(log_blue_green_ratio).cache().shuffle(batch_size * 4),\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x, log_y = test.map(log_blue_green_ratio).rebatch(test_size).get_single_element()\n",
    "log_y_hat = network(log_x)[:, 0].numpy()\n",
    "log_y = log_y[:, 0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y - log_y_hat)**2).sum()/((log_y - log_y.mean())**2).sum()\n",
    "print(f'R2: {R2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = (\n",
    "    hv.HexTiles((log_y_hat, log_y), kdims=[pred, true], vdims=cbar)\n",
    "    * hv.Slope(1, 0)\n",
    ")\n",
    "plt = plt.options('HexTiles', logz=True, colorbar=True, aspect=1, fontscale=1.4)\n",
    "plt = plt.options('Slope', color='red', line_width=2)\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def four_wavelengths(x, y):\n",
    "    return (\n",
    "        tf.experimental.numpy.log10(\n",
    "            tf.concat(\n",
    "                (tf.gather(x, blue, axis=1), tf.gather(x, [green], axis=1)),\n",
    "                axis=1,\n",
    "            )\n",
    "        ),\n",
    "        tf.experimental.numpy.log10(\n",
    "            tf.reduce_sum(y, axis=1)\n",
    "        ),\n",
    "    )    \n",
    "\n",
    "network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, 'relu'),\n",
    "    tf.keras.layers.Dense(64, 'relu'),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "network.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    ")\n",
    "train_cached = train.map(four_wavelengths).cache()\n",
    "for _ in train_cached: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = network.fit(\n",
    "    train_cache.shuffle(batch_size * 4),\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x, log_y = test.map(four_wavelengths).rebatch(test_size).get_single_element()\n",
    "log_y_hat = network(log_x)[:, 0].numpy()\n",
    "log_y = log_y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y - log_y_hat)**2).sum()/((log_y - log_y.mean())**2).sum()\n",
    "print(f'R2: {R2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = hv.Dimension('prediction', range=(-6, 3))\n",
    "plt = (\n",
    "    hv.HexTiles((log_y_hat, log_y), kdims=[pred, true], vdims=count)\n",
    "    * hv.Slope(1, 0)\n",
    ")\n",
    "plt = plt.options('HexTiles', logz=True, colorbar=True, aspect=1, fontscale=1.4)\n",
    "plt = plt.options('Slope', color='red', line_width=2)\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlp, loc scale out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, 'relu'),\n",
    "    tf.keras.layers.Dense(64, 'relu'),\n",
    "    # tf.keras.layers.Dense(64, 'relu'),\n",
    "    tf.keras.layers.Dense(2),\n",
    "    tfp.layers.IndependentNormal(1),    \n",
    "])\n",
    "network.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=lambda y, model: tf.reduce_sum(-model.log_prob(y)),\n",
    ")\n",
    "train_cache = train.map(four_wavelengths).cache()\n",
    "for _ in train_cache: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = network.fit(\n",
    "    train_cache.shuffle(batch_size * 4),\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_x, log_y = test.map(four_wavelengths).rebatch(test_size).get_single_element()\n",
    "log_y_model = network(log_x)\n",
    "log_y_hat = log_y_model.mean()[:, 0].numpy()\n",
    "log_y = log_y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y - log_y_hat)**2).sum()/((log_y - log_y.mean())**2).sum()\n",
    "print(f'R2: {R2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = (\n",
    "#    hv.HexTiles((log_y_hat, log_y), kdims=['pred', 'true'], vdims='cbar')\n",
    "    hv.HexTiles((log_y_hat, log_y), kdims=[pred, true], vdims=cbar)\n",
    "    * hv.Slope(1, 0)\n",
    ")\n",
    "plt = plt.options('HexTiles', logz=True, colorbar=True, aspect=1, fontscale=1.4)\n",
    "plt = plt.options('Slope', color='red', line_width=2)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (log_y_model.stddev() / tf.abs(log_y_model.mean()) < 0.5)[:, 0].numpy()\n",
    "idx.sum() / idx.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1 - ((log_y[idx] - log_y_hat[idx])**2).sum()/((log_y[idx] - log_y[idx].mean())**2).sum()\n",
    "print(f'R2: {R2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = (\n",
    "    hv.HexTiles((log_y_hat[idx], log_y[idx]), kdims=[pred, true], vdims=cbar)\n",
    "    * hv.Slope(1, 0)\n",
    ")\n",
    "plt = plt.options('HexTiles', logz=True, colorbar=True, aspect=1, fontscale=1.4)\n",
    "plt = plt.options('Slope', color='red', line_width=2)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_norm = (log_y - log_y_model.mean()[:, 0].numpy()) / log_y_model.stddev()[:, 0].numpy()\n",
    "ecdf = kit.ecdf(std_norm)\n",
    "by = 1000\n",
    "x = np.linspace(-5, 9, 80)\n",
    "y = tfp.distributions.Normal(0, 1).cdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdims = r'$$\\text{log}(\\textit{chl-a}\\ [\\mathit{mg}\\ m^{-3}])$$'\n",
    "vdims = r'$$\\text{eCDF}$$'\n",
    "plt = (\n",
    "    hv.Scatter(\n",
    "        (std_norm[::by], ecdf[::by]),\n",
    "        kdims=kdims,\n",
    "        vdims=vdims,\n",
    "    ).options(color='black')\n",
    "    * hv.Curve((x, y)).options(color='red')\n",
    ")\n",
    "plt.options(fontscale=1.4, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectum with Taxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(train.shuffle(32).take(4).as_numpy_iterator())\n",
    "(hv.Curve(x) + hv.Bars(y)).opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Outdated Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUK41jPxQHeT"
   },
   "source": [
    "## Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZUWOMYQgYBK"
   },
   "source": [
    "The features and labels are both model output from NASA GMAO using the [NOBM and OASIM](https://gmao.gsfc.nasa.gov/gmaoftp/NOBM) models. The labels are four phytoplankton chlorophyll densities output by NOBM. The features are normalized water leaving radiances output by OASIM, using the NOBM model as input."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocess = importlib.reload(preprocess)\n",
    "kit = importlib.reload(kit)\n",
    "HyperLwn = preprocess.HyperLwn\n",
    "PhytoChl = preprocess.PhytoChl\n",
    "\n",
    "sample = xr.open_dataset(kit.DATA_DIR/'sample.nc')\n",
    "sample['pxl'] = range(sample.sizes['pxl'])\n",
    "sample['labels'] = (\n",
    "    sample[kit.TAXA]\n",
    "    .to_array(dim='component')\n",
    "    .transpose('pxl', 'component', ...)\n",
    ")\n",
    "sample_n = (sample - sample.mean('pxl')) #/sample.std('pxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0gHiexEgq3T"
   },
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CNGKtURguXF"
   },
   "source": [
    "One NetCDF file contains all the predictor data. Note that the `FillValue` attribute is not set to `9.99e11` in the netCDF file (Cecile will fix in next version). There are no explicit coordinates given; they are documented as attributes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ncdump -h {os.environ['PWD']}/data/nobm/HyperLwn.R2014.nc4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nonnull_grid = int((~HyperLwn.isel(wavelength=0, month=0).isnull()).sum())\n",
    "Markdown(f\"\"\"\n",
    "Variable `HyperLwn` has non-null values at {nonnull_grid:,} pixels for each month\n",
    "and wavelength.\n",
    "\n",
    "In total, that gives {nonnull_grid * HyperLwn.sizes['month']:,} samples (that are highly non-independent!).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nonnull = int(HyperLwn.size - HyperLwn.isnull().sum())\n",
    "Markdown(f\"\"\"\n",
    "Augmented with coordinates, variable `HyperLwn` is a xarray.DataArray with {nonnull:,} values.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HyperLwn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpkjtJG-iRTW"
   },
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFz1ooc6iaCY"
   },
   "source": [
    "Each of twelve NetCDF files contain a month of NOBM model output. The first is representative. Unlike the HyperLwn file, this one contains coordinates."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ncdump -h {os.environ['PWD']}/data/nobm/monthly/mon200701.R2014.nc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PhytoChl` xarray.Dataset includes the different phytoplankton groups as variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PhytoChl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLvJkZOpjPzn"
   },
   "source": [
    "## Plot your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaHaKBVujTGO"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The radiances currently make a nice map, but the data should be more sparsely sampled."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dmap = (\n",
    "    HyperLwn\n",
    "    .sel(month=[2, 6, 10], wavelength=[465, 665], method='nearest')\n",
    "    .hvplot.image(\n",
    "        groupby=['month', 'wavelength'],\n",
    "        subplots=True,\n",
    "        clabel='Lwn (mW cm-2 microm-1 sr-1)',\n",
    "        rasterize=True,\n",
    "    )\n",
    "    .opts(shared_axes=False)\n",
    ")\n",
    "dmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few \"typical\" hyperspectral radiances."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dmap = (\n",
    "    HyperLwn\n",
    "    .sel({'lon': -120, 'lat': -15, 'month': [2, 6, 10]}, method='nearest')\n",
    "    .hvplot\n",
    "    .line(by='month', ylabel='Lwn')\n",
    "    # * hv.Slope(0, -0.2).options(color=hv.dim('wavelength'))\n",
    ")\n",
    "dmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean centered radiances and corresponding phytoplankton abundances."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pxl = [4, 34, 53, 283]\n",
    "grays = ['#000000', '#444444', '#777777', '#aaaaaa']\n",
    "pigments = ['#47AC5F', '#FBEC2C', '#F884AB', '#E93429']\n",
    "line = (\n",
    "    sample['features'].sel(pxl=pxl)\n",
    "    .hvplot\n",
    "    .line(x='wavelength', by='pxl', ylabel='Lwn', legend=True)\n",
    "    .options('Curve', fontscale=1.4, color=hv.Cycle(grays))\n",
    "    .options('NdOverlay', legend_position='top_right')\n",
    ")\n",
    "(\n",
    "    line\n",
    "    + (\n",
    "        sample['labels']\n",
    "        .reset_coords(drop=True)\n",
    "        .isel(pxl=pxl)\n",
    "        .hvplot.bar(by='component')\n",
    "        .options('Bars', fontscale=1.4, color=hv.Cycle(pigments))\n",
    "    )\n",
    ").cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD to reduce the wavelength dimension to `k` vectors accounting for the most variation in the features. The singular values are:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k = 5\n",
    "scores, s, vectors = kit.svd(sample_n['features'], dim='wavelength', k=k)\n",
    "list(s.round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding vectors:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectors.hvplot.line(x='wavelength', by='pc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix of univariate (diagonal) and bivariate (off-diagonal) histograms of the `scores`, or coefficients generating each wavelength by linear combination of the `vectors` above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\n",
    "    hvplot.scatter_matrix(\n",
    "        scores.to_dataset(dim='pc').to_dataframe(),\n",
    "        chart='hexbin',\n",
    "        gridsize=16,\n",
    "    )\n",
    "    .opts(hv.opts.HexTiles(cmap='Viridis', tools=['hover']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEXjgfVPjspm"
   },
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A map of the phytoplankton labels in `PhytoChl` at one month."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\n",
    "    PhytoChl\n",
    "    .sel(month=[2, 5, 8, 11])\n",
    "    .hvplot.image(\n",
    "        z=kit.TAXA,\n",
    "        groupby=['month'],\n",
    "        subplots=True,\n",
    "        clabel='chl-a',\n",
    "        rasterize=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the four phytoplankton groups."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample['labels_p'] = (sample['labels'].dims, kit.ecdf(sample['labels']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\n",
    "    sample[['labels', 'labels_p']]\n",
    "    .drop_vars('pxl')\n",
    "    .hvplot\n",
    "#    .line(x='labels', y='labels_p', by='component')\n",
    "#    .opts(hv.opts.Curve(interpolation='steps-pre'))\n",
    "    .scatter(x='labels', y='labels_p', by='component', xlabel='chl-a', ylabel='probability')\n",
    "    .opts(title='ECDF of phytoplankton by component')\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scores, s, vectors = kit.svd(sample_n['labels'], dim='component')\n",
    "s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.cov(scores, rowvar=False).round(8)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels = xr.Dataset({\n",
    "    'scores': scores,\n",
    "    'scores_p': (scores.dims, kit.ecdf(scores)),\n",
    "})\n",
    "(\n",
    "    labels[['scores', 'scores_p']]\n",
    "    .hvplot\n",
    "#    .line(x='labels', y='labels_p', by='component')\n",
    "#    .opts(hv.opts.Curve(interpolation='steps-pre'))\n",
    "    .scatter(x='scores', y='scores_p', by='pc', xlabel='score', ylabel='probability')\n",
    "    .opts(title='ECDF of phytoplankton PCA by component')\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\n",
    "    hvplot.scatter_matrix(\n",
    "        scores.to_dataset(dim='pc').to_dataframe(),\n",
    "        chart='hexbin',\n",
    "        gridsize=16,\n",
    "    )\n",
    "    .opts(hv.opts.HexTiles(cmap='Viridis', tools=['hover']))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
