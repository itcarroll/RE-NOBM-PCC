stages:
  notebooks:
    foreach:
    - preview
    do:
      cmd: >
        jupyter nbconvert
        --execute
        --ExecutePreprocessor.kernel_name=re-nobm-pcc
        --to=html
        --output-dir=docs
        notebooks/${item}.ipynb
      outs:
      - docs/${item}.html
  simulate:
    cmd:
    - mkdir data/rrs_day
    - >
        srun
        --job-name=re-nobm-pcc.simulate
        --output="logs/slurm-%x-%3t.out"
        --ntasks=288
        --mem-per-cpu=20G
        --exclude=poseidon-compute-13
        task re_nobm_pcc.simulate
    deps:
    - data/nobm_day
    - data/oasim_param
    outs:
    - data/rrs_day
  preprocess:
    # prepare the features and labels for model training
    # on gpu for tf analysis?
    cmd: srun --gres=gpu:1 python -m re_nobm_pcc.preprocess
    deps:
    - data/nobm
    - re_nobm_pcc/kit.py
    - re_nobm_pcc/preprocess.py
    outs:
    - data/train
    - data/validate
    - data/test
    - data/sample.nc
  learn:
    # train the model
    cmd: srun --gres=gpu:1 python -m re_nobm_pcc.learn
    deps:
    - data/train
    - data/validate
    - data/test
    - re_nobm_pcc/kit.py
    - re_nobm_pcc/learn.py
    outs:
    - data/model
    - data/fit.npz
    metrics:
    - data/metrics.json:
        cache: false
  evaluate:
    # generate report on model performance
    cmd: jupyter nbconvert
      --execute
      --no-input
      --to html
      --output evaluate.html
      notebooks/evaluate.ipynb
    deps:
    - notebooks/evaluate.ipynb
    - data/model
    - data/fit.npz
    - data/metrics.json
    outs:
    - notebooks/evaluate.html
